---
title: "SVD"
output: html_notebook
---

```{r}
library(ggplot2)
library(caret)
library(quanteda)
library(doSNOW)
library(irlba)
library(randomForest)
library(lsa)
```


```{r}
# Function for calculating relative term frequency (TF)
term_freq <- function(row) {
  row / sum(row)
}

# Function for calculating inverse document frequency (IDF)
inverse_doc_freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))
  
  log10(corpus.size / doc.count)
}

# Function for calculating TF-IDF.
tf_idf <- function(tf, idf) {
  tf * idf
}

get_bag_of_words <- function(data) {
  # Tokenize SMS text messages.
  data_tokens <- tokens(data, what = "word",
                         remove_numbers = TRUE, remove_punct = TRUE,
                         remove_symbols = TRUE, remove_hyphens = TRUE)
  
  # Lower case the tokens.
  data_tokens <- tokens_tolower(data_tokens)
  
  # Use quanteda's built-in stopword list for English.
  # NOTE - You should always inspect stopword lists for applicability to
  #        your problem/domain.
  data_tokens <- tokens_select(data_tokens, stopwords(), 
                                selection = "remove")
  
  # Perform stemming on the tokens.
  data_tokens <- tokens_wordstem(data_tokens, language = "english")
  
  # Add bigrams to our feature matrix.
  data_tokens <- tokens_ngrams(data_tokens, n = 1:2)
  
  # Create bag-of-words model.
  data_tokens_dfm <- dfm(data_tokens, tolower = FALSE)
  return(data_tokens_dfm)
}

get_tf_idf <- function(data_tokens_dfm) {
  # Transform to a matrix.
  data_tokens_matrix <- as.matrix(data_tokens_dfm)
  
  # Normalize all documents via TF.
  data_tokens_tf <- apply(data_tokens_matrix, 1, term_freq)
  
  # Calculate the IDF vector that we will use - both
  # for training data and for test data!
  data_tokens_idf <- apply(data_tokens_matrix, 2, inverse_doc_freq)
  
  # Calculate TF-IDF for our training corpus.
  data_tokens_tf_idf <-  apply(data_tokens_tf, 2, tf_idf, idf = data_tokens_idf)
  
  # Transpose the matrix
  data_tokens_tf_idf <- t(data_tokens_tf_idf)
  
  result <- list("data_tokens_tf_idf" = data_tokens_tf_idf, "data_tokens_idf" = data_tokens_idf)
  
  return(result)
}

fix_incomplete_cases <- function(data_tokens_tf_idf) {
  # Check for incopmlete cases.
  incomplete_cases <- which(!complete.cases(data_tokens_tf_idf))
  
  # Fix incomplete cases
  data_tokens_tf_idf[incomplete_cases,] <- rep(0.0, ncol(data_tokens_tf_idf))
  
  return(data_tokens_tf_idf)
}

apply_idf <- function(test_tokens_dfm, train_tokens_idf) {
  test_tokens_matrix <- as.matrix(test_tokens_dfm)
  
  # Normalize all documents via TF.
  test_tokens_tf <- apply(test_tokens_matrix, 1, term_freq)
  
  # Calculate TF-IDF for our training corpus.
  test_tokens_tf_idf <-  apply(test_tokens_tf, 2, tf_idf, idf = train_tokens_idf)
  
  # Transpose the matrix
  test_tokens_tf_idf <- t(test_tokens_tf_idf)
  
  return(test_tokens_tf_idf)
}

pre_process_text_data <- function(train, test) {
  
  train_tokens_dfm = get_bag_of_words(train)
  train_tokens_tf_idf = get_tf_idf(train_tokens_dfm)
  train_tokens_tf_idf$data_tokens_tf_idf = fix_incomplete_cases(train_tokens_tf_idf$data_tokens_tf_idf)
  
  test_tokens_dfm = get_bag_of_words(test)
  test_tokens_dfm <- dfm_select(test_tokens_dfm, pattern = train_tokens_dfm,
                                selection = "keep")
  test_tokens_tf_idf = apply_idf(test_tokens_dfm, train_tokens_tf_idf$data_tokens_idf)
  test_tokens_tf_idf = fix_incomplete_cases(test_tokens_tf_idf)
  
  result <- list(
    "train_tokens_tf_idf" = train_tokens_tf_idf$data_tokens_tf_idf,
    "test_tokens_tf_idf" = test_tokens_tf_idf)
  
  return(result)
}

perform_svd <- function(data, features_count) {
  # Perform SVD.
  data_irlba <- irlba(t(data), nv = features_count, maxit = (2 * features_count))
  sigma_inverse <- 1 / data_irlba$d
  u_transpose <- t(data_irlba$u)
  
  result <- list("v" = data_irlba$v, "sigma_inverse" = sigma_inverse, "u_transpose" = u_transpose)
  
  return(result)
}
```

```{r}
prepare_movie_data <- function() {
  
}

prepare_spam_data <- function() {
  # Load up the .CSV data and explore in RStudio.
  spam.raw <- read.csv("spam.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
  
  # Clean up the data frame and view our handiwork.
  spam.raw <- spam.raw[, 1:2]
  names(spam.raw) <- c("Label", "Text")
  
  # Check data to see if there are missing values.
  length(which(!complete.cases(spam.raw)))
  
  # Convert our class label into a factor.
  spam.raw$Label <- as.factor(spam.raw$Label)
  
  # Use caret to create a 70%/30% stratified split. Set the random
  # seed for reproducibility.
  set.seed(32984)
  indexes <- createDataPartition(spam.raw$Label, times = 1,
                                 p = 0.7, list = FALSE)
  
  train <- spam.raw[indexes,]
  test <- spam.raw[-indexes,]
  
  preprocess_result = pre_process_text_data(train$Text, test$Text)
  train_tokens_tf_idf = preprocess_result$train_tokens_tf_idf
  test_tokens_tf_idf = preprocess_result$test_tokens_tf_idf
  
  result <- list(
    "train" = train, "test" = test,
    "train_tokens_tf_idf" = train_tokens_tf_idf,  "test_tokens_tf_idf" = test_tokens_tf_idf)
  
  return(result)
}

write_spam_data_to_csv <- function(data) {
  train_df <- data.frame(Label = data$train$Label, data$train_tokens_tf_idf)
  names(train_df) <- make.names(names(train_df))
  test_df <- data.frame(Label = data$test$Label, data$test_tokens_tf_idf)
  names(test_df) <- make.names(names(test_df))
  write.csv(train_df, file = "spam_train_bag_of_words_with_svd.csv", fileEncoding = "UTF-8")
  write.csv(test_df, file = "spam_test_bag_of_words_with_svd.csv", fileEncoding = "UTF-8")
}

write_movie_data_to_csv <- function(data) {
  train_df <- data.frame(Label = data$train$Label, data$train_tokens_tf_idf)
  names(train_df) <- make.names(names(train_df))
  test_df <- data.frame(Label = data$test$Label, data$test_tokens_tf_idf)
  names(test_df) <- make.names(names(test_df))
  write.csv(train_df, file = "movie_train_bag_of_words_with_svd.csv", fileEncoding = "UTF-8")
  write.csv(test_df, file = "movie_test_bag_of_words_with_svd.csv", fileEncoding = "UTF-8")
}

load_prepared_spam_data <- function() {
  train <- read.csv("spam_train_bag_of_words.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
  test <- read.csv("spam_test_bag_of_words.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
  train_tokens_tf_idf = as.matrix(train[,-(1:2)])
  test_tokens_tf_idf = as.matrix(test[,-(1:2)])
  # Convert our class label into a factor.
  train$Label <- as.factor(train$Label)
  test$Label <- as.factor(test$Label)
  result <- list(
    "train" = train, "test" = test,
    "train_tokens_tf_idf" = train_tokens_tf_idf,  "test_tokens_tf_idf" = test_tokens_tf_idf)
  
  return(result)
}

load_prepared_movie_data <- function() {
  train <- read.csv("movies_for_svd_train.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8", header = FALSE)
  test <- read.csv("movies_for_svd_test.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8", header = FALSE)
  train_tokens_tf_idf = as.matrix(train[,-c(1, ncol(train))])
  test_tokens_tf_idf = as.matrix(test[,-c(1, ncol(test))])
  # Convert our class label into a factor.
  train$Label <- as.factor(train[,ncol(train)])
  test$Label <- as.factor(test[,ncol(test)])
  result <- list(
    "train" = train, "test" = test,
    "train_tokens_tf_idf" = train_tokens_tf_idf,  "test_tokens_tf_idf" = test_tokens_tf_idf)

  return(result)
}
```

```{r}
# Preprocess data and save (without SVD)
data = prepare_movie_data()
spam_data = prepare_spam_data()
write_spam_data_to_csv(spam_data)
```

```{r}
# Load preprocessed data Spam
prepared_spam_data = load_prepared_spam_data()
```

```{r}
#  Apply SVD Spam
modified_spam_data = prepared_spam_data
train_tokens_svd = perform_svd(modified_spam_data$train_tokens_tf_idf, 50)
modified_spam_data$train_tokens_tf_idf = train_tokens_svd$v
modified_spam_data$test_tokens_tf_idf = t(
  train_tokens_svd$sigma_inverse * train_tokens_svd$u_transpose %*% t(modified_spam_data$test_tokens_tf_idf))

write_spam_data_to_csv(modified_spam_data)
```

```{r}
# Load preprocessed data Movie
prepared_movie_data = load_prepared_movie_data()
```

```{r}
#  Apply SVD Movie
modified_movie_data = prepared_movie_data
train_tokens_svd = perform_svd(modified_movie_data$train_tokens_tf_idf, 50)
modified_movie_data$train_tokens_tf_idf = train_tokens_svd$v
modified_movie_data$test_tokens_tf_idf = t(
  train_tokens_svd$sigma_inverse * train_tokens_svd$u_transpose %*% t(modified_movie_data$test_tokens_tf_idf))

write_movie_data_to_csv(modified_movie_data)
```
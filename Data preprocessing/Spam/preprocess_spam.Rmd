---
title: "Preprocessing"
output: html_notebook
---

```{r}
library(ggplot2)
library(caret)
library(quanteda)
library(doSNOW)
library(irlba)
library(randomForest)
library(lsa)
```


```{r}
# Function for calculating relative term frequency (TF)
term_freq <- function(row) {
  row / sum(row)
}

# Function for calculating inverse document frequency (IDF)
inverse_doc_freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))
  
  log10(corpus.size / doc.count)
}

# Function for calculating TF-IDF.
tf_idf <- function(tf, idf) {
  tf * idf
}

get_bag_of_words <- function(data) {
  # Tokenize SMS text messages.
  data_tokens <- tokens(data, what = "word",
                         remove_numbers = TRUE, remove_punct = TRUE,
                         remove_symbols = TRUE, remove_hyphens = TRUE)
  
  # Lower case the tokens.
  data_tokens <- tokens_tolower(data_tokens)
  
  # Use quanteda's built-in stopword list for English.
  # NOTE - You should always inspect stopword lists for applicability to
  #        your problem/domain.
  data_tokens <- tokens_select(data_tokens, stopwords(), 
                                selection = "remove")
  
  # Perform stemming on the tokens.
  data_tokens <- tokens_wordstem(data_tokens, language = "english")
  
  # Add bigrams to our feature matrix.
  data_tokens <- tokens_ngrams(data_tokens, n = 1:2)
  
  # Create bag-of-words model.
  data_tokens_dfm <- dfm(data_tokens, tolower = FALSE)
  return(data_tokens_dfm)
}

get_tf_idf <- function(data_tokens_dfm) {
  # Transform to a matrix.
  data_tokens_matrix <- as.matrix(data_tokens_dfm)
  
  # Normalize all documents via TF.
  data_tokens_tf <- apply(data_tokens_matrix, 1, term_freq)
  
  # Calculate the IDF vector that we will use - both
  # for training data and for test data!
  data_tokens_idf <- apply(data_tokens_matrix, 2, inverse_doc_freq)
  
  # Calculate TF-IDF for our training corpus.
  data_tokens_tf_idf <-  apply(data_tokens_tf, 2, tf_idf, idf = data_tokens_idf)
  
  # Transpose the matrix
  data_tokens_tf_idf <- t(data_tokens_tf_idf)
  
  result <- list("data_tokens_tf_idf" = data_tokens_tf_idf, "data_tokens_idf" = data_tokens_idf)
  
  return(result)
}

fix_incomplete_cases <- function(data_tokens_tf_idf) {
  # Check for incopmlete cases.
  incomplete_cases <- which(!complete.cases(data_tokens_tf_idf))
  
  # Fix incomplete cases
  data_tokens_tf_idf[incomplete_cases,] <- rep(0.0, ncol(data_tokens_tf_idf))
  
  return(data_tokens_tf_idf)
}

apply_idf <- function(test_tokens_dfm, train_tokens_idf) {
  test_tokens_matrix <- as.matrix(test_tokens_dfm)
  
  # Normalize all documents via TF.
  test_tokens_tf <- apply(test_tokens_matrix, 1, term_freq)
  
  # Calculate TF-IDF for our training corpus.
  test_tokens_tf_idf <-  apply(test_tokens_tf, 2, tf_idf, idf = train_tokens_idf)
  
  # Transpose the matrix
  test_tokens_tf_idf <- t(test_tokens_tf_idf)
  
  return(test_tokens_tf_idf)
}

pre_process_text_data <- function(train, test) {
  
  train_tokens_dfm = get_bag_of_words(train)
  train_tokens_tf_idf = get_tf_idf(train_tokens_dfm)
  train_tokens_tf_idf$data_tokens_tf_idf = fix_incomplete_cases(train_tokens_tf_idf$data_tokens_tf_idf)
  
  test_tokens_dfm = get_bag_of_words(test)
  test_tokens_dfm <- dfm_select(test_tokens_dfm, pattern = train_tokens_dfm,
                                selection = "keep")
  test_tokens_tf_idf = apply_idf(test_tokens_dfm, train_tokens_tf_idf$data_tokens_idf)
  test_tokens_tf_idf = fix_incomplete_cases(test_tokens_tf_idf)
  
  result <- list(
    "train_tokens_tf_idf" = train_tokens_tf_idf$data_tokens_tf_idf,
    "test_tokens_tf_idf" = test_tokens_tf_idf)
  
  return(result)
}
```

```{r}
prepare_spam_data <- function() {
  # Load up the .CSV data and explore in RStudio.
  spam.raw <- read.csv("spam.csv", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
  
  # Clean up the data frame and view our handiwork.
  spam.raw <- spam.raw[, 1:2]
  names(spam.raw) <- c("Label", "Text")
  
  # Check data to see if there are missing values.
  length(which(!complete.cases(spam.raw)))
  
  # Convert our class label into a factor.
  spam.raw$Label <- as.factor(spam.raw$Label)
  
  # Use caret to create a 70%/30% stratified split. Set the random
  # seed for reproducibility.
  set.seed(32984)
  indexes <- createDataPartition(spam.raw$Label, times = 1,
                                 p = 0.7, list = FALSE)
  
  train <- spam.raw[indexes,]
  test <- spam.raw[-indexes,]
  
  preprocess_result = pre_process_text_data(train$Text, test$Text)
  train_tokens_tf_idf = preprocess_result$train_tokens_tf_idf
  test_tokens_tf_idf = preprocess_result$test_tokens_tf_idf
  
  result <- list(
    "train" = train, "test" = test,
    "train_tokens_tf_idf" = train_tokens_tf_idf,  "test_tokens_tf_idf" = test_tokens_tf_idf)
  
  return(result)
}

write_spam_data_to_csv <- function(data) {
  train_df <- data.frame(Label = data$train$Label, data$train_tokens_tf_idf)
  names(train_df) <- make.names(names(train_df))
  test_df <- data.frame(Label = data$test$Label, data$test_tokens_tf_idf)
  names(test_df) <- make.names(names(test_df))
  write.csv(train_df, file = "spam_train_bag_of_words.csv", fileEncoding = "UTF-8")
  write.csv(test_df, file = "spam_test_bag_of_words.csv", fileEncoding = "UTF-8")
}
```

```{r}
# Preprocess data and save (without SVD)
spam_data = prepare_spam_data()
write_spam_data_to_csv(spam_data)
```
